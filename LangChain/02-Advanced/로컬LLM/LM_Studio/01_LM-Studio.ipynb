{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "#   base_url=\"https://usually-adjusted-raptor.ngrok-free.app/v1\",\n",
    "  base_url=\"http://localhost:1234/v1\",\n",
    "  api_key=\"lm-studio\",\n",
    "#   model=\"teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf\",\n",
    "  model=\"Qwen/Qwen2-7B-Instruct-GGUF\",\n",
    "  streaming=True,\n",
    "  callbacks=[StreamingStdOutCallbackHandler()],\n",
    "  temperature=0\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "  \"\"\"You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.\n",
    "  You must generate an answer in Korean.\n",
    "  \n",
    "  #Question:\n",
    "  {question}\n",
    "  \n",
    "  #Answer: \n",
    "  \"\"\"\n",
    ")\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하지만 기본적으로 Q4 이하를 목표로 하며, Nvidia의 cuBLAS 또는 AMD의 rocBLAS을 사용하고 있다면 I-quants을 고려해야 합니다. 이런 것은 IQX_X 형식으로 제공되며, IQ3_M과 같이 표시됩니다. 이것은 더 새로운 것이고, 그 크기에 비해 성능이 훨씬 좋습니다.\n",
      "\n",
      "I-quants는 또한 CPU 및 Apple Metal에서 사용할 수 있지만, 그들의 K-quant 동등한 버전보다 느리므로 속도와 성능의 트레이드오프가 필요합니다.\n",
      "\n",
      "Vulcan과 호환되지 않는 I-quants을 사용하므로 AMD 카드를 사용하고 있다면 rocBLAS 빌드 또는 Vulcan 빌드를 사용하고 있는지 확인해야 합니다. 현재 작성 중인 문서에서 LM Studio는 ROCm 지원의 미리보기 버전이 있으며, 다른 인퍼런스 엔진은 ROCm을 위한 특정 빌드를 가지고 있습니다."
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "  \"\"\"너는 번역을 도와주는 AI야. 아래 내용을 한국어로 번역해줘\n",
    "  \n",
    "  {question}\n",
    "  \"\"\"\n",
    ")\n",
    "question = \"\"\"\n",
    "But basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\n",
    "\n",
    "These I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\n",
    "\n",
    "The I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 서울의 면적은 대략 605.21제곱킬로미터(km²)입니다. 이는 한강을 기준으로 북쪽으로는 성동구, 동쪽으로는 광진구와 중랑구, 남쪽으로는 동작구와 관악구, 서쪽으로는 은평구와 마포구에 둘러싸인 지역으로 구성되어 있습니다. 이 면적에는 도심 지역뿐만 아니라 주거 및 상업 지구, 공원, 그리고 한강과 같은 자연 환경도 포함되어 있습니다."
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"서울의 면적은?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
