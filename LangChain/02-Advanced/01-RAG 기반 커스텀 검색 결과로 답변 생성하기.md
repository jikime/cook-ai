**ì£¼ìš”ë‚´ìš©**
- ğŸ›  `.env` íŒŒì¼ì„ ì‚¬ìš©í•œ **í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬**ë¡œ ì½”ë“œ ë³´ì•ˆ ê°•í™”
- ğŸ“ **DirectoryLoader**ì™€ **PyPDFLoader**ë¥¼ í†µí•œ ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œ ë¡œë“œ
- ğŸ¤– **langchain_openai** ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ **ì–¸ì–´ ëª¨ë¸**ê³¼ **í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿**ì˜ ì‚¬ìš©
- ğŸ” **ë¹„ë™ê¸° ê²€ìƒ‰**ê³¼ **BM25Retriever**ë¡œ íš¨ìœ¨ì ì¸ ë¬¸ì„œ ê²€ìƒ‰ ë° ê²°ê³¼ ë³‘í•©
- ğŸ’¡ **LLM**ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ì¿¼ë¦¬ ìƒì„± ë° ë‹µë³€ ì¶”ì¶œ

`load_dotenv` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•¨ìœ¼ë¡œì¨, `.env` íŒŒì¼ ë‚´ì— ì •ì˜ëœ í™˜ê²½ ë³€ìˆ˜ë“¤ì´ í”„ë¡œê·¸ë¨ì˜ í™˜ê²½ ë³€ìˆ˜ë¡œ ë¡œë“œë©ë‹ˆë‹¤. ì´ëŠ” ë³´ì•ˆì´ ì¤‘ìš”í•œ ì •ë³´(ì˜ˆ: ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë°€ë²ˆí˜¸, API í‚¤ ë“±)ë¥¼ ì½”ë“œì— ì§ì ‘ í•˜ë“œì½”ë”©í•˜ì§€ ì•Šê³  ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” íš¨ê³¼ì ì¸ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.



```python
# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
from dotenv import load_dotenv

load_dotenv()
```

<br>``DirectoryLoader``ëŠ” ì£¼ë¡œ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë° ì‚¬ìš©ë˜ì§€ë§Œ, ``glob`` ë§¤ê°œë³€ìˆ˜ì™€ ``loader_cls`` ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ ë‹¤ë¥¸ í˜•ì‹ì˜ íŒŒì¼ë„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ``PyPDFLoader``ë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ``folder_path`` ë³€ìˆ˜ë¥¼ í†µí•´ ë¡œë“œí•  íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ì´ ê²½ìš°, ì‚¬ì—…ë³´ê³ ì„œê°€ í¬í•¨ëœ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  PDF íŒŒì¼(``**/*.pdf``)ì„ ë¡œë“œí•˜ê¸° ìœ„í•´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.



```python
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader

folder_path = './data/ì‚¬ì—…ë³´ê³ ì„œ'
loader = DirectoryLoader(folder_path, glob="**/*.pdf", loader_cls=PyPDFLoader)
documents = loader.load()
```

<br>ì•„ë˜ ì½”ë“œëŠ” `langchain_openai` ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ `ChatOpenAI`ì™€ `OpenAIEmbeddings` í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤. `ChatOpenAI` ì¸ìŠ¤í„´ìŠ¤ëŠ” `gpt-4o-mini` ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©°, `temperature`ë¥¼ 0.1ë¡œ ì„¤ì •í•˜ì—¬ ìƒì„±ë©ë‹ˆë‹¤. ì´ëŠ” ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ ì˜ˆì¸¡ ê°€ëŠ¥ì„±ì„ ë†’ì´ê³ , ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. `OpenAIEmbeddings` ì¸ìŠ¤í„´ìŠ¤ëŠ” `text-embedding-3-small` ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©°, ë°ì´í„° ì²˜ë¦¬ ì‹œ ì²­í¬ í¬ê¸°ë¥¼ 256ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ìƒì„±ë©ë‹ˆë‹¤. ì´ëŠ” í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.



```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
embed_model = OpenAIEmbeddings(
    model="text-embedding-3-small",
    chunk_size=256
)
```

<br>ì•„ë˜ í•¨ìˆ˜ëŠ” ë¬¸ì„œë¥¼ ë¶„í• í•˜ê³ , ë¶„í• ëœ ë¬¸ì„œë“¤ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. `split_docs` í•¨ìˆ˜ëŠ” `RecursiveCharacterTextSplitter`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ íŠ¹ì • í¬ê¸°(`chunk_size`)ì™€ ê²¹ì¹¨(`chunk_overlap`)ì„ ê°€ì§„ ì—¬ëŸ¬ ë¶€ë¶„ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤. ì´í›„, `Chroma.from_documents` í•¨ìˆ˜ëŠ” ë¶„í• ëœ ë¬¸ì„œë“¤ê³¼ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³ , ì´ë¥¼ ë²¡í„° í˜•íƒœë¡œ ì €ì¥í•˜ì—¬ ë” íš¨ìœ¨ì ì¸ ê²€ìƒ‰, ë¶„ë¥˜, ë˜ëŠ” ë‹¤ë¥¸ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.



```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

def split_docs(documents, chunk_size=1024, chunk_overlap=20):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap,
    )
    docs = text_splitter.split_documents(documents)
    return docs

docs = split_docs(documents)

vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embed_model
)
```

<br>ì•„ë˜ ì½”ë“œëŠ” `langchain.prompts`ì—ì„œ `PromptTemplate` í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì™€ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì œê³µí•œ ì§ˆë¬¸ì— ëŒ€í•´ AIê°€ ì—¬ëŸ¬ ê°œì˜ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤. ì´ í…œí”Œë¦¿ì€ ì‚¬ìš©ìê°€ ì§€ì •í•œ ì§ˆë¬¸(`query_string`)ì„ ê¸°ë°˜ìœ¼ë¡œ, ì§€ì •ëœ ê°œìˆ˜(`count`)ë§Œí¼ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. `PromptTemplate` í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•  ë•ŒëŠ” ì…ë ¥ ë³€ìˆ˜ë¡œ ì§ˆë¬¸ì˜ ê°œìˆ˜ì™€ ì§ˆë¬¸ ë‚´ìš©ì„ í¬í•¨í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.



```python
from langchain_core.prompts import PromptTemplate

query_string = "í˜„ì¬ ì‚¼ì„±ì „ìì˜ ìµœëŒ€ ì£¼ì£¼ëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"

query_prompt = """
ë„ˆëŠ” ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ì—¬ëŸ¬ê°œì˜ ì§ˆë¬¸ë¥¼ ìƒì„±í•˜ëŠ” AIì•¼.
ì£¼ì–´ì§„ ì§ˆë¬¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” {count} ê°œì˜ ìƒˆë¡œìš´ ì§ˆë¬¸ë¥¼ ë§Œë“¤ì–´ì¤˜.
í•œ ì¤„ì— í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ ì‘ì„±í•´ì£¼ê³  ì¸ë±ìŠ¤ ë²ˆí˜¸ëŠ” í•„ìš”ì—†ì–´.
ì§ˆë¬¸: {query}
ìƒˆë¡œìš´ ì§ˆë¬¸:
"""

prompt_template = PromptTemplate(
    input_variables=["count", "query"],
    template=query_prompt
)
```

<br>ì•„ë˜ í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ì¿¼ë¦¬ ë¬¸ìì—´ê³¼ ìƒì„±í•  ì¿¼ë¦¬ì˜ ìˆ˜ë¥¼ ì¸ìë¡œ ë°›ì•„, LLM(Large Language Model)ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ì¿¼ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤. í•¨ìˆ˜ëŠ” ë¨¼ì € ì£¼ì–´ì§„ ì¿¼ë¦¬ë¥¼ í¬í•¨í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ LLMì— ì „ë‹¬í•˜ì—¬ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤. ì‘ë‹µëœ ë‚´ìš©ì€ ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• ë˜ì–´, ê° ì¿¼ë¦¬ê°€ ê³µë°± ì—†ì´ ì •ë¦¬ë©ë‹ˆë‹¤. ê·¸ í›„, ì›ë³¸ ì¿¼ë¦¬ë¥¼ ìƒì„±ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ í•­ëª©ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ìš”ì²­ëœ ì¿¼ë¦¬ ìˆ˜ë§Œí¼ì˜ ì¿¼ë¦¬ë¥¼ ë°˜í™˜í•˜ê¸° ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì ì ˆíˆ ì˜ë¼ëƒ…ë‹ˆë‹¤. ì´ ê³¼ì •ì„ í†µí•´, ì‚¬ìš©ìëŠ” ì›í•˜ëŠ” ìˆ˜ì˜ ê´€ë ¨ ì¿¼ë¦¬ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.



```python
from typing import List

def generate_queries(query: str, count: int = 5) -> List[str]:
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = prompt_template.format(
        count=count - 1, 
        query=query
    )
    
    # LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
    response = llm.invoke(prompt)
    
    # ì‘ë‹µì„ ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• 
    generated_queries = response.content.split("\n")
    cleaned_queries = [query.strip() for query in generated_queries]
    
    # ì›ë³¸ ì¿¼ë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ í•­ëª©ìœ¼ë¡œ ì¶”ê°€
    all_queries = [query] + cleaned_queries
    
    # ìƒì„±ëœ ì¿¼ë¦¬ ìˆ˜ê°€ ìš”ì²­ëœ ìˆ˜ë³´ë‹¤ ë§ì€ ê²½ìš° ì˜ë¼ë‚´ê¸°
    return all_queries[:count]
```

<br>í•¨ìˆ˜ `generate_queries`ëŠ” ì£¼ì–´ì§„ `query_string`ì„ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ ìˆ˜ì˜ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” `query_string`ì„ ê¸°ë°˜ìœ¼ë¡œ 5ê°œì˜ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ê²€ìƒ‰ ì¿¼ë¦¬, ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìƒì„± ë“± ë‹¤ì–‘í•œ ëª©ì ìœ¼ë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



```python
# query_string ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ 5ê°œì˜ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
queries = generate_queries(query_string, 5)
# ìƒì„±ëœ ì¿¼ë¦¬ë“¤ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
print(queries)
```

<br>ì•„ë˜ í•¨ìˆ˜ëŠ” ë¹„ë™ê¸°ì ìœ¼ë¡œ ì—¬ëŸ¬ ê²€ìƒ‰ê¸°(`retrievers`)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì¿¼ë¦¬(`queries`)ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. ê° ì¿¼ë¦¬ì™€ ê²€ìƒ‰ê¸° ì¡°í•©ì— ëŒ€í•´ ë³„ë„ì˜ ë¹„ë™ê¸° ì‘ì—…ì„ ìƒì„±í•˜ê³ , `asyncio.to_thread`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì‘ì—…ì˜ ì‹¤í–‰ì€ `tqdm`ì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒí™©ì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•˜ë©°, ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ë©´ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.



```python
from tqdm.asyncio import tqdm
import asyncio

async def run_queries(queries, retrievers):
    # ê° ì¿¼ë¦¬ì™€ ê²€ìƒ‰ê¸°ì— ëŒ€í•´ ë¹„ë™ê¸° ì‘ì—…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    tasks = []
    for query in queries:
        for retriever in retrievers:
            tasks.append(
                asyncio.create_task(asyncio.to_thread(retriever.invoke, query))
            )

    # ëª¨ë“  ì‘ì—…ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    task_results = await tqdm.gather(*tasks)

    return task_results
```

<br>ì•„ë˜ ì½”ë“œëŠ” `langchain.retrievers`ì—ì„œ `BM25Retriever`ë¥¼ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤. ë¨¼ì €, `vectorstore`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ì„± ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„° ê²€ìƒ‰ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ìµœê·¼ì ‘ ì´ì›ƒ ê²€ìƒ‰ì— ì‚¬ìš©ë  ì´ì›ƒì˜ ìˆ˜(`k`)ë¥¼ 3ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ, `BM25Retriever`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì§‘í•©ì—ì„œ BM25 ê²€ìƒ‰ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì´ ê²€ìƒ‰ê¸° ì—­ì‹œ `k` ê°’ì„ 3ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ìƒìœ„ 3ê°œì˜ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë°˜í™˜í•˜ë„ë¡ í•©ë‹ˆë‹¤.



```python
from langchain_community.retrievers import BM25Retriever

# ë²¡í„° ê²€ìƒ‰ê¸° ì„¤ì •
vector_retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3},
)

# BM25 ê²€ìƒ‰ê¸° ì„¤ì •
bm25_retriever = BM25Retriever.from_documents(docs)
bm25_retriever.k = 3
```

<br>ì•„ë˜ í•¨ìˆ˜ëŠ” ì—¬ëŸ¬ ë¬¸ì„œ ì§‘í•©ì—ì„œ ê° ë¬¸ì„œì˜ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ `k`ê°œì˜ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì„ íƒí•˜ì—¬ ë³‘í•©í•©ë‹ˆë‹¤. `merge_results_by_similarity_score` í•¨ìˆ˜ëŠ” ë‘ ë§¤ê°œë³€ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤: `results`, ì—¬ëŸ¬ ë¬¸ì„œ ì§‘í•©ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‚˜íƒ€ë‚´ë©°, ê° ë¬¸ì„œëŠ” `Document` ê°ì²´ì…ë‹ˆë‹¤; `similarity_top_k`, ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œì˜ ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤(ê¸°ë³¸ê°’ì€ 5). ê° `Document` ê°ì²´ì˜ `page_content`ë¥¼ í‚¤ë¡œ í•˜ê³ , `metadata`ì—ì„œ `score`ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•˜ì—¬ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë³‘í•©í•©ë‹ˆë‹¤. ì´í›„, ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ë¬¸ì„œë¥¼ ì •ë ¬í•˜ê³  ìƒìœ„ `k`ê°œ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.



```python
from langchain_core.documents import Document

def merge_results_by_similarity_score(
    results: List[List[Document]], similarity_top_k: int = 5
) -> List[Document]:
    # ìœ ì‚¬ë„ ì ìˆ˜ì— ë”°ë¼ ê²°ê³¼ë¥¼ ë³‘í•©í•˜ëŠ” í•¨ìˆ˜
    merge_scores = {}
    for docs in results:
        for doc in docs:
            # ë¬¸ì„œì˜ í˜ì´ì§€ ë‚´ìš©ì´ merge_scoresì— ì—†ìœ¼ë©´ ì¶”ê°€
            if doc.page_content not in merge_scores:
                merge_scores[doc.page_content] = doc.metadata.get("score", 0.0)
            else:
                # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìµœëŒ€ ì ìˆ˜ë¥¼ ìœ ì§€
                merge_scores[doc.page_content] = max(
                    merge_scores[doc.page_content], doc.metadata.get("score", 0.0)
                )

    # ì ìˆ˜ì— ë”°ë¼ ê²°ê³¼ë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
    ranked_results = sorted(merge_scores.items(), key=lambda x: x[1], reverse=True)
    # ìƒìœ„ kê°œì˜ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ë°˜í™˜
    return [
        Document(page_content=content, metadata={"score": score})
        for content, score in ranked_results[:similarity_top_k]
    ]
```

<br>ì•„ë˜ í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ì¿¼ë¦¬ ë¬¸ìì—´ì— ê¸°ë°˜í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³ , ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. `generate_queries` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì¿¼ë¦¬ ë¬¸ìì—´ë¡œë¶€í„° ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´í›„, `run_queries` í•¨ìˆ˜ë¥¼ í†µí•´ ìƒì„±ëœ ì¿¼ë¦¬ë“¤ì„ ì‹¤í–‰í•˜ê³ , `vector_retriever`ì™€ `bm25_retriever` ê²€ìƒ‰ ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤. ì–»ì€ ê²°ê³¼ëŠ” `merge_results_by_similarity_score` í•¨ìˆ˜ë¥¼ í†µí•´ ìœ ì‚¬ë„ ì ìˆ˜ì— ë”°ë¼ ë³‘í•©ë˜ë©°, ìƒìœ„ `similarity_top_k`ê°œì˜ ë¬¸ì„œë§Œ ìµœì¢… ê²°ê³¼ë¡œ ì„ íƒë©ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ, ì„ íƒëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì€ ìˆœì„œëŒ€ë¡œ ë¬¸ìì—´ì— ì¶”ê°€ë˜ì–´ ë°˜í™˜ë©ë‹ˆë‹¤.



```python
async def get_related_docs(
    query_string: str, num_generate_query: int = 3, similarity_top_k: int = 3
) -> str:
    # ì£¼ì–´ì§„ ì¿¼ë¦¬ ë¬¸ìì—´ë¡œë¶€í„° ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    queries = generate_queries(query_string, num_generate_query)
    # ìƒì„±ëœ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤.
    task_results = await run_queries(queries, [vector_retriever, bm25_retriever])
    # ìœ ì‚¬ë„ ì ìˆ˜ì— ë”°ë¼ ê²°ê³¼ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.
    final_results = merge_results_by_similarity_score(task_results, similarity_top_k)

    # ìµœì¢… ê²°ê³¼ë¡œë¶€í„° ê´€ë ¨ ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
    related_docs = ""
    for i, doc in enumerate(final_results):
        related_docs += f"\n[{i}]: {doc.page_content}\n"
    return related_docs
```

<br>ì•„ë˜ í•¨ìˆ˜ëŠ” ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ `get_related_docs` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ íŠ¹ì • ì¿¼ë¦¬ ë¬¸ìì—´ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë¬¸ì„œëŠ” `docs` ë³€ìˆ˜ì— ì €ì¥ë˜ë©°, ì´í›„ `print` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. `query_string` ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ ê²€ìƒ‰í•˜ê³ ì í•˜ëŠ” ì¿¼ë¦¬ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.



```python
# ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
docs = await get_related_docs(query_string="ì‚¼ì„±ì „ìì˜ DXë¶€ë¬¸ ë§¤ì¶œì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?")
# ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
print(docs)
```

<br>ì•„ë˜ í•¨ìˆ˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸(`query`)ì„ ë°›ì•„, ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ì— ì§ˆë¬¸ì„ ì œì‹œí•˜ì—¬ ë‹µë³€ì„ ì–»ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ì…ë‹ˆë‹¤. ë¨¼ì €, `get_related_docs` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì´ë•Œ, ê²€ìƒ‰í•  ì¿¼ë¦¬ì˜ ìˆ˜(`num_generate_query`)ì™€ ìœ ì‚¬ë„ê°€ ë†’ì€ ìƒìœ„ ë¬¸ì„œì˜ ìˆ˜(`similarity_top_k`)ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë¬¸ì„œëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±ì— ì‚¬ìš©ë˜ë©°, ì´ í”„ë¡¬í”„íŠ¸ëŠ” ì–¸ì–´ ëª¨ë¸(`llm`)ì„ í˜¸ì¶œí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ì–¸ì–´ ëª¨ë¸ì˜ ì‘ë‹µ(`response.content`)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.



```python
async def answer(query):
  # ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
  docs = await get_related_docs(query_string=query, num_generate_query=3, similarity_top_k=10)
  # ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  prompt = f"""
  ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜

  ë¬¸ì„œ
  {docs}

  ì§ˆë¬¸: {query}
  """
  # ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
  response = llm.invoke(prompt)
  # ì–¸ì–´ ëª¨ë¸ì˜ ì‘ë‹µ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
  return response.content
```

<br>ì•„ë˜ ì½”ë“œëŠ” `query_list`ì— ì €ì¥ëœ ì§ˆë¬¸ ëª©ë¡ì„ ìˆœíšŒí•˜ë©° ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìš”ì²­í•©ë‹ˆë‹¤. ê° ì§ˆë¬¸(`q`)ì— ëŒ€í•´, ì§ˆë¬¸ì„ ì¶œë ¥í•˜ê³  `answer(q)` í•¨ìˆ˜ë¥¼ í†µí•´ ì–»ì€ ë‹µë³€ì„ ì¶œë ¥í•œ í›„, êµ¬ë¶„ì„ ì„ ì¶œë ¥í•©ë‹ˆë‹¤. `answer` í•¨ìˆ˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ ê°€ì •ë˜ë©°, ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.



```python
query_list = ["í˜„ì¬ ì‚¼ì„±ì „ìì˜ ìµœëŒ€ ì£¼ì£¼ëŠ” ëˆ„êµ¬ì¸ê°€ìš”?", "ì‚¼ì„±ì „ìì˜ DXë¶€ë¬¸ ë§¤ì¶œì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?"]

for q in query_list:
  print(f"Q: {q}")
  print(f"A: {await answer(q)}")
  print("\n=========================\n")
```
